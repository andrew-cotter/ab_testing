---
title: "A/B Testing in R Markdown"
author: "Andrew Cotter"
date: "2023-10-25"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


## Introduction

### Scenario

A fast-food chain plans to add a new item to its menu. However, they are still undecided between three possible promotion campaigns for promoting the new product. In order to determine which promotion has the greatest effect on sales, the new item is introduced at locations in several randomly selected markets. A different promotion is used at each location, and the weekly sales of the new item are recorded for the first four weeks.

### Goal

Evaluate A/B testing results and decide which promotion strategy works the best.

### Columns

-   **MarketID**: unique identifier for market
-   **MarketSize**: size of market area by sales
-   **LocationID**: unique identifier for store location
-   **AgeOfStore**: age of store in years 
-   **Promotion**: one of three promotions that were tested
-   **week**: one of four weeks when the promotions were run
-   **SalesInThousands**: sales amount for a specific LocationID, Promotion, and week

Data was sourced from [Kaggle](https://www.kaggle.com/datasets/chebotinaa/fast-food-promotion-campaign-ab-test)

### Outcome

Employing a linear mixed-effect model, conditioned on Market ID and Market Size, the findings indicate that Promotion #1 emerges as the most effective campaign for advancing. Projections suggest that this campaign is poised to yield average sales outcomes ranging from 6% to 10% superior to the next most favorable alternative.

------------------------------------------------------------------------

## Data Loading and Inspection

```{r}
library(readxl)
d = read.csv("WA_Marketing-Campaign.csv")
head(d)
```

```{r}
#Redefinee numerics as factors
d$MarketID = as.factor(d$MarketID)
d$LocationID = as.factor(d$LocationID)
d$Promotion = as.factor(d$Promotion)

#Column Summaries
print(summary(d))
```

```{r}
#Missing value check
table(is.na(d))
```

Upon inspection, adjustments were made to redefine the data types of MarketID, LocationID, and Promotion as factors. 

------------------------------------------------------------------------

## Searching for Confounds

The following sections explore the variables in the data set to assess whether or not they are confounds that will need to be accounted for in the final model.

A *confound* is any variable, whether it is in the data set or not, that impacts  the treatment and the outcome. This commonly arises from poor experimental design, such as imbalances in the characteristics of the control and treatment groups.

### Distribution of Promotions Across Markets

```{r}
#Proportion table for percentage of promotions within each market
round(
  prop.table(
    table(d$MarketID, d$Promotion),
    margin = 1),
  2
)
#Chi-square test for differences in promotion distribution across markets
chisq.test(table(d$MarketID, d$Promotion))
```

```{r}
library(dplyr)
library(ggplot2)
#Average Sales per MarketID
d%>%
  group_by(MarketID)%>%
  summarise(avg_sales = mean(SalesInThousands)) %>%
  ggplot(aes(x = MarketID, y = avg_sales))+
    geom_bar(stat = "identity")+
    theme_classic()

#ANOVA to test for significant differences in sales across MarketIDs
summary(aov(SalesInThousands~MarketID, d))
```

*MarketID* is a confounding factor. The promotions are not equally distributed across the different markets (as indicated by the significant chi-square statistic), and the market has a significant impact on sales (as indicated by the ANOVA).

### Market Size

NOTE: Each *MarketID*, which was already determined a confound, only has a single market size value associated with it. If market size has a significant impact on sales, it would explain some of why MarketID has an impact on sales.

```{r}
d %>%
  group_by(MarketSize) %>%
  summarise(avg_sales = mean(SalesInThousands))

summary(aov(SalesInThousands~MarketSize, d))
```

```{r}
#Proportion table for percentage of market sizes per promotion type
round(
  prop.table(
    table(d$Promotion, d$MarketSize),
    margin = 1),
  2
)
#Chi-Square test for equality of observations per cell
chisq.test(table(d$Promotion, d$MarketSize))
```
The 1-way ANOVA yielded statistical significance, and the summary table shows that large markets see, on average, about 60% more in sales  compared to medium markets. However, it cannot be concluded that the promotions are differently distributed across the different market sizes. Therefore, *MarketSize* is not a confound. 


### Store Age

```{r}
library(ggplot2)
#Average for each store across the 4 weeks
d %>% group_by(LocationID) %>%
  summarise(AgeOfStore = mean(AgeOfStore), Avg_Sales = mean(SalesInThousands)) %>%
#Plot
ggplot(aes(x = AgeOfStore, y = Avg_Sales))+
  geom_jitter(width = 0.2, alpha = 0.2, color = "blue", size = 3)+
  theme_classic()+
  geom_smooth(method = "lm", color = "black")+
  ggtitle("Average Sales by Store Age")+
  xlab("Age of Store (Years)")+ylab("Average Sales (Thousands)")
cor.test(d$AgeOfStore, d$SalesInThousands)
```

It doesn't appear that the age of the store has a significant impact on sales.

### Time

```{r}
#Visualization of sales over the Promotional Trial.
ggplot(d, aes(x = week, y = SalesInThousands, group = LocationID, color = MarketSize))+
  geom_line(alpha = 0.4, size = 1)+
  theme_classic()+
  ggtitle("Sales During Promotion Campaigns (First 4 Weeks)")
```
```{r}
library(reshape2)
#Reshape to 1 row per LocationID, weeks as columns, sales as cell values
dcast = dcast(d, LocationID~week, value.var = "SalesInThousands")
#Create a new column that represents the difference in sales between weeks 1 and 4
dcast$diff = dcast[,5]-dcast[,2]

#Density plot of differences
ggplot(dcast, aes(diff))+
  geom_density(color = "blue", size = 1, fill = "blue", alpha = 0.5, bw = 1)+
  geom_histogram(aes(y = ..density..), fill = "black", alpha = 0.3)+
  theme_classic()+
  geom_vline(aes(xintercept = median(dcast$diff)), size = 1.2)+
  annotate(
    "text", 
    x = median(dcast$diff)-0.5, y = 0.065, 
    label = paste("median = ", median(dcast$diff)), 
    angle = 90, fontface = "bold")+
  xlab("Difference in Individual Location Sales (Week 4 - Week 1)")
```
```{r}
library(lme4)
summary(lmer(SalesInThousands~week+(1|MarketID/LocationID), d))
```

All of the visualizations and analysis techniques attempted above suggest no clear trend in sales over the four weeks.

## Analysis of Promotion Campaign

It has been established that *MarketID* impacts both *MarketSize* and average sales, and that the *Promotions* are not equally distributed across the different markets. The casual structure is visualized in the directed acyclic graph (DAG) below:

```{r, echo = FALSE}
library(ggdag)
coords = list(
  x = c(MarketID = 0, Promotion = 0, SalesInThousands = 0, MarketSize = 0.1),
  y = c(MarketID = 10, Promotion = 5, SalesInThousands = 0, MarketSize = 5)
)
dagify(
  SalesInThousands~Promotion, 
  Promotion~MarketID,
  MarketSize~MarketID,
  SalesInThousands~MarketSize,
  labels = c(
    "SalesInThousands" = "SalesInThousands",
    "Promotion" = "Promotion",
    "MarketSize" = "MarketSize",
    "MarketID" = "MarketID"
  ),
  exposure = "Promotion", 
  outcome = "SalesInThousands", 
  coords = coords
) %>% ggdag(node_size = 0, text_size = 4, text_col = "blue")+
  theme_dag()+
  xlim(-0.1, 0.2)
```

The final model is going to need to be conditioned on the treatment (*Promotion*) and the outcome (*SalesInThousands*). However, this variable set alone leaves a backdoor path from promotion to sales via the route that goes through *MarketID* and *MarketSize*.

Additionally, *MarketSize* affects sales independent of *Promotion*. Conditioning the model on *MarketSize* will take care of both of these concerns - it will isolate the effect of *Promotion* by removing variability in sales due to *MarketSize* while also closing the backdoor path between *Promotion* and sales via *MarketID*. Finally, nested random intercepts will also be included.


```{r}
m1 = lmer(SalesInThousands ~ Promotion + MarketSize + (1|MarketID/LocationID), d)
```

```{r}
library(emmeans)
#Extracting summary statistics and estimates from the model
model_output = emmeans(m1, specs = pairwise~Promotion)
estimates = data.frame(model_output$emmeans)

#Plotting model estimates
ggplot(estimates, aes(x = Promotion, y = emmean, fill = Promotion, label = round(emmean,1)))+
  geom_bar(stat = "identity")+
  geom_errorbar(
    aes(ymin = lower.CL, ymax = upper.CL),
    width = 0.3,
    size = 1.3
    )+
  geom_text(size = 5, nudge_x = .35, nudge_y = -1.6, color = "white", fontface = "bold")+
  theme_classic()+
  scale_fill_brewer(palette = "Dark2")+
  ylab("Average Sales (Thousands)")+
  ggtitle("Average Sales by Promotion")+
  theme(legend.position = "None")
```

```{r}
#Pairwise Comparisons
model_output$contrasts
```
## Discussion

This model suggests that Promotion 1 is the superior campaign, followed by Promotion 3 and then Promotion 2. All of the individual pairwise comparisons were significantly different, indicating that each one produces a distinctly different value in terms of average sales.

It may appear at first glance that the error bars on the graph and the pairwise comparisons are not aligned in terms of their conclusions. The degree of overlap in the error bars suggests a lack of statistical significance, while the pairwise comparisons extracted from the model all have very low p-values. In this case, however, these two metrics are calculated differently and have distinct interpretations:

-   The error bars communicate the degree of uncertainty we have about the sales yielded by each promotional campaign, in reference to the average across a population of markets. There were 10 markets in this experiment, and the assumption was made that these 10 markets represent a subset of a broader population of markets. Due to the inherent variability in sales that can be attributed to the markets, these estimates have a relatively high uncertainty associated with them.
-   Considering the pairwise comparisons, it is worth remembering that, for the most part, each market saw each promotional campaign at least one time. Consequently, when it comes to comparing one campaign to another, the model is doing those comparisons *within each market* first. From there, the model is using that distribution within-market comparisons to assess the significance of the difference between two promotional campaigns. This gives the pairwise comparisons more statistical power because the model is able to control for the inter-market variability that is not controlled for in the estimated means.
