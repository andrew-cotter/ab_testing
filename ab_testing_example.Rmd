---
title: "A/B Testing in R Markdown"
author: "Andrew Cotter"
date: "2023-10-25"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


## Introduction

### Scenario

A fast-food chain plans to add a new item to its menu. However, they are still undecided between three possible promotion campaigns for promoting the new product. In order to determine which promotion has the greatest effect on sales, the new item is introduced at locations in several randomly selected markets. A different promotion is used at each location, and the weekly sales of the new item are recorded for the first four weeks.

### Goal

Evaluate A/B testing results and decide which promotion strategy works the best.

### Columns

-   **MarketID**: unique identifier for market
-   **MarketSize**: size of market area by sales
-   **LocationID**: unique identifier for store location
-   **AgeOfStore**: age of store in years Promotion: one of three promotions that were tested
-   **week**: one of four weeks when the promotions were run
-   **SalesInThousands**: sales amount for a specific LocationID, Promotion, and week

Data was sourced from [Kaggle](https://www.kaggle.com/datasets/chebotinaa/fast-food-promotion-campaign-ab-test)

------------------------------------------------------------------------

## Data Loading and Inspection

```{r}
library(readxl)
d = read.csv("WA_Marketing-Campaign.csv")
head(d)
```

Upon inspection, I notice that there are a few columns that need redefining in terms of their data type. MarketID, LocationID, and Promotion should all be factors rather than integers.

```{r}
d$MarketID = as.factor(d$MarketID)
d$LocationID = as.factor(d$LocationID)
d$Promotion = as.factor(d$Promotion)

print(summary(d))
print(range(table(d$MarketID)))
```

It appears that there are some imbalances in the data set.

-   For MarketID, there are a variety of different counts that range from 24-88. The number of observations that we have per market is going to influence the degree of confidence in the conclusions that we can draw within that market.
-   In terms of market sizes, it appears that medium markets are by far the most common. Again, we are going to have less confidence in the conclusions that we draw about large and especially small markets compared to the ones we draw about medium markets.
-   Finally, it appears that promotion 1 was shown slightly less than promotions 2 and 3.

```{r}
table(is.na(d))
```

There are no missing values.

------------------------------------------------------------------------

## Searching for Confounds

Before we go about building a model for our variable of interest (Promotion), we need to do some checks to ensure that there are no *confounds* that were introduced by the experimental design.

A *confound* is any variable, whether it is in the data set or not, that impacts both the treatment and the outcome. This commonly arises from poor experimental design, such as imbalances in the characteristics of the control and treatment groups.

### Distribution of Promotions Across Markets

First we will look at how the different treatments (Promotions) were distributed across the markets, and if there is significant variability in the outcome (SalesInThousands) that can be attributed to the different markets. If both of these criteria are satisfied, then *MarketID* is a confound.

```{r}
#Proportion table for percentage of promotions within each market
round(
  prop.table(
    table(d$MarketID, d$Promotion),
    margin = 1),
  2
)
#Chi-square test for differences in promotion distribution across markets
chisq.test(table(d$MarketID, d$Promotion))
```

```{r}
library(dplyr)
library(ggplot2)
#Average Sales per MarketID
d%>%
  group_by(MarketID)%>%
  summarise(avg_sales = mean(SalesInThousands)) %>%
  ggplot(aes(x = MarketID, y = avg_sales))+
    geom_bar(stat = "identity")+
    theme_classic()

#ANOVA to test for significant differences in sales across MarketIDs
summary(aov(SalesInThousands~MarketID, d))
```

*MarketID* is a confounding factor. The promotions are not equally distributed across the different markets (as indicated by the significant chi-square statistic), and the market has a significant impact on sales (as indicated by the ANOVA).

### Market Size

Next, we will investigate *MarketSize*. It is worth pointing out here that each *MarketID*, which we already established is a confound, only has a single market size value associated with it. If we can show that market size has a significant impact on sales, that would explain at least some of why MarketID has an impact on sales.

```{r}
d %>%
  group_by(MarketSize) %>%
  summarise(avg_sales = mean(SalesInThousands))

summary(aov(SalesInThousands~MarketSize, d))
```

The data is suggesting that the market size has an appreciable impact on sales. The 1-way ANOVA suggests statistical significance, and the summary table shows that large markets see, on average, about 60% more in sales when compared to medium markets.

Next, we need to see if the promotion campaigns are unequally distributed across the different market sizes.

```{r}
#Proportion table for percentage of market sizes per promotion type
round(
  prop.table(
    table(d$Promotion, d$MarketSize),
    margin = 1),
  2
)
#Chi-Square test for equality of observations per cell
chisq.test(table(d$Promotion, d$MarketSize))
```

Since the chi-square statistic is not significant, we fail to reject the null hypothesis and conclude that the promotions are equally distributed across the different market sizes.

Up to now, we have established that *MarketID* impacts both *MarketSize* and average sales, and that the *Promotions* are not equally distributed across the different markets. Keeping these in mind, we will move onto investigating some of the other measured variables.

### Store Age

Next, in our further exploration of possible confounds, we can check to see if the age of the store has an impact on sales. A simple scatter plot and correlation metric will suffice here.

```{r}
library(ggplot2)
#Average for each store across the 4 weeks
d %>% group_by(LocationID) %>%
  summarise(AgeOfStore = mean(AgeOfStore), Avg_Sales = mean(SalesInThousands)) %>%
#Plot
ggplot(aes(x = AgeOfStore, y = Avg_Sales))+
  geom_jitter(width = 0.2, alpha = 0.2, color = "blue", size = 3)+
  theme_classic()+
  geom_smooth(method = "lm", color = "black")+
  ggtitle("Average Sales by Store Age")+
  xlab("Age of Store (Years)")+ylab("Average Sales (Thousands)")
cor.test(d$AgeOfStore, d$SalesInThousands)
```

It doesn't appear that the age of the store has a significant impact on sales.

### Time

Getting into our first look at the efficacy of the promotion campaigns, let's visualize how the store sales change over the course of the 4 weeks that were measured. One would assume that a successful promotion campaign would show increased sales as time goes on. However, there are plenty of market forces outside of our control that could impact these numbers at large.

```{r}
ggplot(d, aes(x = week, y = SalesInThousands, group = LocationID, color = MarketSize))+
  geom_line(alpha = 0.4, size = 1)+
  theme_classic()+
  ggtitle("Sales During promotion Campaigns (First 4 Weeks)")
```

There is a lot going on here, and it is tough to parse out any trend over time, likely meaning that any effect is going to be relatively small and potentially insignificant. I decided to color the lines by market size, as they further illustrate that:

-   Large markets tend to have much higher sales numbers than small and medium markets.
-   Medium markets far outnumber both small and large markets in this data set.

I will employ a slightly different approach to investigate whether sales are increasing over time.

```{r}
library(reshape2)
#Reshape to 1 row per LocationID, weeks as columns, sales as cell values
dcast = dcast(d, LocationID~week, value.var = "SalesInThousands")
#Create a new column that represents the difference in sales between weeks 1 and 4
dcast$diff = dcast[,5]-dcast[,2]

#Density plot of differences
ggplot(dcast, aes(diff))+
  geom_density(color = "blue", size = 1, fill = "blue", alpha = 0.5, bw = 1)+
  geom_histogram(aes(y = ..density..), fill = "black", alpha = 0.3)+
  theme_classic()+
  geom_vline(aes(xintercept = median(dcast$diff)), size = 1.2)+
  annotate(
    "text", 
    x = median(dcast$diff)-0.5, y = 0.065, 
    label = paste("median = ", median(dcast$diff)), 
    angle = 90, fontface = "bold")+
  xlab("Difference in Individual Location Earnings (Week 4 - Week 1)")
```

There doesn't seem to be much of a noteworthy trend here, either. If anything, this distribution is slightly skewed to suggest fewer sales in Week 4. As one final test, we can write a regression model to check this conclusion.

For this model, I will be predicting sales as a function of week. Some of the features of this model include:

- Random intercepts for each LocationID, since the locations represent a small sample of a larger population.
- Additionally, locations nested within markets, which are also random, so I will specify that in the model as well.
- Random slopes for each promotion, in case the effect over time varies by promotion.

```{r}
library(lme4)
summary(lmer(SalesInThousands~week+(Promotion|MarketID/LocationID), d))
```

Looking at the summary statistics of this model, week is given a very small negative coefficient with a relatively large standard error. This means that week has a small, insignificant impact on sales. We won't need to worry about it much when assessing the impact of the promotion campaigns.

## Analysis of Promotion Campaign

Before we start constructing a model to analyze the effect of *Promotion* on *SalesInThousands*, we need to consider what potential confounds we encountered in the exercises above.

To reiterate, we established that *MarketID* impacts both *MarketSize* and average sales, and that the *Promotions* are not equally distributed across the different markets. Visually, the causal structure looks like this:

```{r, echo = FALSE}
library(ggdag)
coords = list(
  x = c(MarketID = 0, Promotion = 0, SalesInThousands = 0, MarketSize = 0.1),
  y = c(MarketID = 10, Promotion = 5, SalesInThousands = 0, MarketSize = 5)
)
dagify(
  SalesInThousands~Promotion, 
  Promotion~MarketID,
  MarketSize~MarketID,
  SalesInThousands~MarketSize,
  labels = c(
    "SalesInThousands" = "SalesInThousands",
    "Promotion" = "Promotion",
    "MarketSize" = "MarketSize",
    "MarketID" = "MarketID"
  ),
  exposure = "Promotion", 
  outcome = "SalesInThousands", 
  coords = coords
) %>% ggdag(node_size = 0, text_size = 4, text_col = "blue")+
  theme_dag()+
  xlim(-0.1, 0.2)
```

This directed acyclic graph (DAG) summarizes what we learned about the factors that we need to consider in constructing a model that isolates the effect that promotion has on sales.

Obviously, the model is going to need to be conditioned on the two main variables of interest, the treatment (*Promotion*) and the outcome (*SalesInThousands*). However, if we just leave it at that, there will be a backdoor path from promotion to sales via the route that goes through *MarketID* and *MarketSize*.

Additionally, *MarketSize* affects sales independent from *Promotion*. Conditioning our model on *MarketSize* will take care of both of these concerns - it will isolate the effect of *Promotion* by removing variability in sales due to *MarketSize* while also closing the backdoor path between *Promotion* and sales via *MarketID*.

Finally, we will keep *LocationID* nested in *MarketID* as a random intercepts, and allow random slopes for each *Promotion*.

```{r}
m1 = lmer(SalesInThousands~Promotion + MarketSize + (Promotion|MarketID/LocationID), d)
```

```{r}
library(emmeans)
#Extracting summary statistics and estimates from the model
model_output = emmeans(m1, specs = pairwise~Promotion)
estimates = data.frame(model_output$emmeans)

#Plotting model estimates
ggplot(estimates, aes(x = Promotion, y = emmean, fill = Promotion, label = round(emmean,1)))+
  geom_bar(stat = "identity")+
  geom_errorbar(
    aes(ymin = lower.CL, ymax = upper.CL),
    width = 0.3,
    size = 1.3
    )+
  geom_text(size = 5, nudge_x = .35, nudge_y = -1.6, color = "white", fontface = "bold")+
  theme_classic()+
  scale_fill_brewer(palette = "Dark2")+
  ylab("Average Sales (Thousands)")+
  ggtitle("Average Sales by Promotion")+
  theme(legend.position = "None")
```

```{r}
#Pairwise Comparisons
model_output$contrasts
```

This model suggests that Promotion 1 is the superior campaign, followed by Promotion 3 and then Promotion 2. All of the individual pairwise comparisons were significantly different, indicating that each one produces a distinctly different value in terms of average sales.

It may appear at first glance that the error bars on the graph and the pairwise comparisons are not aligned in terms of their conclusions. The degree of overlap in the error bars suggests a lack of statistical significance, while the pairwise comparisons extracted from the model all have very low p-values. In this case, however, these two metrics are calculated very differently and have different interpretations.

-   The error bars communicate the degree of uncertainty we have about the sales yielded by each promotional campaign, in reference to the average across a population of markets. There were 10 markets in this experiment, and we are assuming that these 10 markets represent a subset of a broader population of markets.
-   When it comes to the pairwise comparisons, it is worth remembering that, for the most part, each market saw each promotional campaign at least one time. Consequently, when it comes to comparing one campaign to another, the model is doing those comparisons *within each market* first. From there, the model is using that distribution within-market comparisons to assess the significance of the difference between two promotional campaigns.
