---
title: "a_b_test"
author: "Andrew Cotter"
date: "2023-10-24"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# A/B Testing Analysis Using an Example Data Set

## Introduction

### Scenario

A fast-food chain plans to add a new item to its menu. However, they are still undecided between three possible promotion campaigns for promoting the new product. In order to determine which promotion has the greatest effect on sales, the new item is introduced at locations in several randomly selected markets. A different promotion is used at each location, and the weekly sales of the new item are recorded for the first four weeks.

### Goal

Evaluate A/B testing results and decide which promotion strategy works the best.

### Columns

-   **MarketID**: unique identifier for market
-   **MarketSize**: size of market area by sales
-   **LocationID**: unique identifier for store location
-   **AgeOfStore**: age of store in years Promotion: one of three promotions that were tested
-   **week**: one of four weeks when the promotions were run
-   **SalesInThousands**: sales amount for a specific LocationID, Promotion, and week

Data was sourced from [Kaggle](https://www.kaggle.com/datasets/chebotinaa/fast-food-promotion-campaign-ab-test)

------------------------------------------------------------------------

## Data Loading and Inspection

```{r}
library(readxl)
d = read.csv("WA_Marketing-Campaign.csv")
head(d)
```

Upon inspection, I notice that there are a few columns that need redefining in terms of their data type. MarketID, LocationID, and Promotion should all be factors rather than integers.

```{r}
d$MarketID = as.factor(d$MarketID)
d$LocationID = as.factor(d$LocationID)
d$Promotion = as.factor(d$Promotion)

print(summary(d))
print(range(table(d$MarketID)))
```

It appears that there are some imbalances in the data set.

-   For MarketID, there are a variety of different counts that range from 24-88. The number of observations that we have per market is going to influence the degree of confidence in the conclusions that we can draw within that market.
-   In terms of market sizes, it appears that medium markets are by far the most common. Again, we are going to have less confidence in the conclusions that we draw about large and especially small markets compared to the ones we draw about medium markets.
-   Finally, it appears that promotion 1 was shown slightly less than promotions 2 and 3.

```{r}
table(is.na(d))
```

There are no missing values.

------------------------------------------------------------------------

## Searching for Confounds

Before we go about building a model for our variable of interest (Promotion), we need to do some checks to ensure that there are no *confounds* that were introduced by the experimental design.

A *confound* is any variable, whether it is in the data set or not, that impacts both the treatment and the outcome. This commonly arises from poor experimental design, such as imbalances in the characteristics of the control and treatment groups.

### Market Size

First, we will investigate market size. To reiterate, we need to establish that market size is a good predictor of both the outcome and the treatment in order to conclude that it is a confound. Starting with its impact on the outcome:

```{r}
library(dplyr)

d %>%
  group_by(MarketSize) %>%
  summarise(avg_sales = mean(SalesInThousands))

(summary(aov(SalesInThousands~MarketSize, d)))
```

The data is suggesting that the market size has an appreciable impact on sales. The 1-way ANOVA suggests statistical significance, and the summary table shows that large markets see, on average, about 60% more in sales when compared to medium markets.

Next, we need to establish that promotion campaigns are unequally distributed across the different market sizes in order to deem market size a confound. Since we have such differences in the number of observations per market, I will use a proportion table to produce the point estimates, and I will use the chi-square test to check for equality across the different cells of the table (of counts).

```{r}
#Proportion table for percentage of market sizes per promotion type
round(
  prop.table(
    table(d$Promotion, d$MarketSize),
    margin = 1),
  2
)

#Chi-Square test for equality of observations per cell
chisq.test(table(d$Promotion, d$MarketSize))
  
```

Based on this analysis, I will not worry too much about market size being a confound. The distribution of promotions is balanced enough across the different markets, based on the insignificance of the chi-square value.

### Store Age

Next, in our further exploration of possible confounds, we can check to see if the age of the store has an impact on sales. A simple scatter plot and correlation metric will suffice here.

```{r}
library(ggplot2)
#Average for each store across the 4 weeks
d %>% group_by(LocationID) %>%
  summarise(AgeOfStore = mean(AgeOfStore), Avg_Sales = mean(SalesInThousands)) %>%
#Plot
ggplot(aes(x = AgeOfStore, y = Avg_Sales))+
  geom_jitter(width = 0.2, alpha = 0.2, color = "blue", size = 3)+
  theme_classic()+
  geom_smooth(method = "lm", color = "black")+
  ggtitle("Average Sales by Store Age")+
  xlab("Age of Store (Years)")+ylab("Average Sales (Thousands)")
cor.test(d$AgeOfStore, d$SalesInThousands)
```

It doesn't appear that the age of the store has a significant impact on sales.

### Time

Getting into our first look at the efficacy of the promotion campaigns, let's visualize how the store sales change over the course of the 4 weeks that were measured. One would assume that a successful promotion campaign would show increased sales as time goes on. However, there are plenty of market forces outside of our control that could impact these numbers at large.

```{r}
ggplot(d, aes(x = week, y = SalesInThousands, group = LocationID, color = MarketSize))+
  geom_line(alpha = 0.4, size = 1)+
  theme_classic()+
  ggtitle("Sales During promotion Campaigns (First 4 Weeks)")
```

There is a lot going on here, and it is tough to parse out any trend over time, likely meaning that any effect is going to be relatively small and potentially insignificant. I decided to color the lines by market size, as they further illustrate that:

-   Large markets tend to have much higher sales numbers than small and medium markets.
-   Medium markets far outnumber both small and large markets in this data set.

I will employ a slightly different approach to investigate whether sales are increasing over time.

```{r}
library(reshape2)
#Reshape to 1 row per LocationID, weeks as columns, sales as cell values
dcast = dcast(d, LocationID~week, value.var = "SalesInThousands")
#Create a new column that represents the difference in sales between weeks 1 and 4
dcast$diff = dcast[,5]-dcast[,2]

#Density plot of differences
ggplot(dcast, aes(diff))+
  geom_density(color = "blue", size = 1, fill = "blue", alpha = 0.5, bw = 1)+
  geom_histogram(aes(y = ..density..), fill = "black", alpha = 0.3)+
  theme_classic()+
  geom_vline(aes(xintercept = median(dcast$diff)), size = 1.2)+
  xlab("Difference in Individual Location Earnings (Week 4 - Week 1)")
```

There doesn't seem to be much of a noteworthy trend here, either. If anything, this distribution is slightly skewed to suggest fewer sales in Week 4. As one final test, we can write a regression model to check this conclusion.

For this model, I will be predicting sales as a function of week. I will be including random intercepts for each LocationID, since the locations represent a small sample of a larger population. Additionally, locations are nested within markets, which are also random, so I will specify that in the model as well. I would ideally like to do random slopes for locations in addition to random intercepts, however I was given warnings about the model being overfit when I tried that.

```{r}
library(lme4)
summary(lmer(SalesInThousands~week+(1|MarketID/LocationID), d))
```

Looking at the summary statistics of this model, week is given a very small negative coefficient with a relatively large standard error. This means that week has a small, insignificant impact on sales. We won't need to worry about it much when assessing the impact of the promotion campaigns

## Analysis of Promotion Campaign

To analyze the impacts of the promotion campaigns, I will use a very similar linear model to the one I used above to investigate the effects of week. The only difference here is that we are replacing *week*, a continuous predictor, with *Promotion*, which is categorical. All other aspects of the model with regards to random effects and nesting will stay the same.

```{r}
model = lmer(SalesInThousands~Promotion + (1|MarketID/LocationID), d)
model_summary = summary(model)

library(lmerTest)
anova(model)
```

```{r}
library(emmeans)
model_output = emmeans(model, specs = pairwise~Promotion)
estimates = data.frame(model_output$emmeans)
ggplot(estimates, aes(x = Promotion, y = emmean, fill = Promotion, label = round(emmean,1)))+
  geom_bar(stat = "identity")+
  geom_errorbar(
    aes(ymin = lower.CL, ymax = upper.CL),
    width = 0.3,
    size = 1.3
    )+
  geom_text(size = 5, nudge_x = .35, nudge_y = -1.6, color = "white", fontface = "bold")+
  theme_classic()+
  scale_fill_brewer(palette = "Dark2")+
  ylab("Average Sales (Thousands)")+
  ggtitle("Average Sales by Promotion")+
  theme(legend.position = "None")
```

```{r}
model_output
```
